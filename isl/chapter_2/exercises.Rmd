---
title: "ISLR Chapter 2 Exercises"
date: 2023-07-01
output:
  md_document:
    toc: true
    toc_depth: 3
  html_document:
    toc: true
    toc_depth: 3
  pdf_document:
    toc: true
    toc_depth: 3

---

```{r, echo = FALSE}
library(knitr)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

## Conceptual

### Question 1

#### a
We would expect a flexible statistical learning method to perform better than
an inflexible method because the risk of overfitting is minimal with a large sample
size and small number of predictors. A flexible method will thus have lower bias
and negligibly higher variance than an inflexible method.

#### b
We would expect an inflexible statistical learning method to perform better than a
flexible method because the flexible method will be at risk of overfitting with a small sample size
and a large number of predictors. The inflexible method will have higher bias but much lower variance
than the flexible method in this case.

#### c
We would expect the flexible method to perform better here because it will be able to
learn the non-linear relationship between the predictors and the response variable better than the inflexible method.
The flexible method will thus have much lower bias than the inflexible method, offsetting the increase in variance.


#### d
We would expect the inflexible method to perform better here because the flexible method
will likely model the large error terms rather than the underlying true relationship between the response
variable and the predictors. The flexible method will have very high variance in this case.

### Question 2

#### a
This is a regression problem, as CEO salary is a continuous variable. We are most
interested in inference here. `n` = 500, `p` = 3.

#### b
This is a classification problem, as the response variable is binary. We are most interested in prediction.
`n` = 20, `p` = 13.

#### c
This is a regression problem, as %change is a continuous variable. We are most interested in prediction.
`n` = 52 (52 weeks in a year), `p` = 3.

### Question 3

[see here](#conceptual)
```{r error_source_graph, }

flexibility <- c(2, 15, 25)

df_bias_squared <- data.frame(
  flexibility = flexibility,
  error = c(1, 0.008, 0.005)
)
df_bias_squared$source <- "bias_squared"

df_variance <- data.frame(
  flexibility = flexibility,
  error = c(0.01, 0.15, 1)
)
df_variance$source <- "variance"

df_training_error <- data.frame(
  flexibility = flexibility,
  error = df_bias_squared$error * 0.8
)
df_training_error$source <- "training_error"

irreducible_error <- 0.5
df_irreducible_error <- data.frame(
  flexibility = c(0, 25),
  error = rep(irreducible_error, 2)
)
df_irreducible_error$source <- "irreducible_error"

df_total_error <- data.frame(
  flexibility = flexibility,
  error = df_bias_squared$error + df_variance$error + df_irreducible_error$error[[1]]
)
df_total_error$source <- "test_error"

df_errors <- dplyr::bind_rows(
  df_bias_squared, df_variance, df_training_error, df_total_error
)

ggplot2::ggplot(df_errors) +
  ggplot2::geom_smooth(ggplot2::aes(x = flexibility, y = error, color = source)) +
  ggplot2::geom_line(data = df_irreducible_error, ggplot2::aes(x = flexibility, y = error, color = source)) +
  ggplot2::labs(x = "Flexibility", y = "Error", title = "Error versus Flexibility for different error sources")
```

Below we describe why each error source has the shape it does.

1. Variance - very non-flexible models, i.e. a model that predicts the mean of the training dataset,
would have close to zero variance; it would barely change from training dataset to training dataset.
In contrast, a very flexible model would change significantly when trained across different datasets, as the flexibility
would result in the model modeling the random error terms of each observation, which change from dataset to dataset. Thus the variance of the error term increases as the flexibility increases. At higher levels of flexibility the slope of the variance curve is higher than at lower levels of flexibility, reflecting the fact that increasing the flexibility when the flexibility
is already quite low doesn't increase the variance that much, but increasing the flexibility when the flexibility is already quite high increases the variance significantly.

2. Bias squared - very non-flexible models, like the mean prediction model mentioned above, have extremely high bias because they are unable to capture the relationship between the predictors and the response variable. As the flexibility increases, the more flexible models are quickly able to capture these relationships, so the error from this term levels off relatively quickly.

3. Irreducible error - this is constant, as it is unaffected by the model chosen and hence is independent of any property of the model, such as flexibility.

4. Test error - this is the sum of the previous three error sources. It achieves a minimum at an intermediate level of flexibility, which is dependent on the dataset that is being modeled. It has a characteristic U shape, reflecting the leveling off of bias at higher levels of flexibility and the rapid increase in variance.

5. Training error - this decreases monotonically as flexibility increases because more complex models approach the point of being able to predict the training response perfectly. For example, a linear model with number of predictors equal to number of observations will be able to perfectly predict the training dataset, provided the features are all linearly independent.

# Applied

Load libraries.
```{r load_libraries}
library(magrittr)
library(dplyr)
library(GGally)
library(ggplot2)
library(tidyr)
library(MASS)
library(ISLR)
```

Summarize data.
```{r summarize_data}
df_college <- College

summary(df_college)

```
