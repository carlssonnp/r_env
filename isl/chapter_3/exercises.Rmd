---
title: ISLR Chapter 3 Exercises
date: 2023-07-01
linkcolor: blue
toccolor: blue
output:
  md_document:
    toc: true
    toc_depth: 3
  html_document:
    toc: true
    toc_depth: 3
  pdf_document:
    toc: true
    toc_depth: 3
---

```{r setup, echo = FALSE}
library(knitr)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r load_libraries}
library(MASS)
library(ggplot2)
library(GGally)
library(dplyr)
library(gridExtra)
library(ISLR)
library(ggfortify)
```

## Conceptual

### Question 1
The null hypothesis for rows 2-4 is that there is no linear relationship between the media type in the leftmost column and
sales. The null hypothesis for the first row (the intercept) is that the mean response is 0 when all media spending is 0.
In terms of coefficients, for each row *i*, indexed starting at 0, the null hypothesis is that $\beta_i = 0$.

### Question 2
The KNN regression method uses the average response of the `k` neighboring points closest to
a test point as the prediction for that point. The KNN classification method first computes the
conditional probability that a test point is of each class by computing the fraction of
the `k` neighboring points that belong to that class, and then assigns the test point to the
class with the maximum conditional probability.

### Question 3

#### a
Answer iii) is correct. The main effect of gender increases female salary relative to
male salary by `35,000`, but the interaction effect decreases female salary by `10,000`
per point of `GPA`. So if a female had a `GPA` of `4.0`, on average they would have a salary that
differs from a male by $35,000 + -10,000 \cdot 4 = -5,000$, provided that the female and male both had the same `IQ` and `GPA`. Since this is a negative number, so we conclude that on average males earn more provided that the GPA is high enough.


#### b
$50 + 20 \cdot 4 + 0.07 \cdot 110 + 35 + 0.01 \cdot 4 \cdot 110 - 10 \cdot 4 \cdot 1 = 137.1$, in thousands of dollars.

#### c
This is false. The t value and the corresponding p-value for the interaction term needs to be examined, rather
than the coefficient value itself. In general, the coefficient value will depend on the scaling of the variable
rather than the significance of the term.  


### Question 4

#### a
The training `RSS` will never increase as more variables are added to the model. This includes
transformations of existing variables. So we would expect the cubic regression to have a lower training
`RSS`.

#### b
Given that the true relationship is linear, the cubic model will not result in a reduction of bias
but will increase variance. So we would expect the linear regression to have a lower test error.

#### c
See part a) above.


#### d
Given that we don't know how far from linear the true relationship is, it is unclear how much bias will be reduced
by using the cubic regression. So we can't tell which method would have lower test `RSS`.


### Question 5

$\frac{\displaystyle\sum_{i=1}^{n} x_iy_i}{\displaystyle\sum_{j=1}^{n} x_j^2} = \displaystyle\sum_{i=1}^{n} \frac{x_i}{\displaystyle\sum_{j=1}^{n} x_j^2} y_i$.

So $\alpha_i = \displaystyle\frac{x_i}{\displaystyle\sum_{j=1}^{n} x_j^2}$

### Question 6
$\hat{y} = \beta_0 + \beta_1x = \bar{y} - \beta_1\bar{x} + \beta_1x = \bar{y} + \beta_1(x - \bar{x})$

This equality holds when $\hat{y} = \bar{y}$ and $x = \bar{x}$.


### Question 7
$\hat{\beta_1} = \frac{\displaystyle\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\displaystyle\sum_{i=1}^{n}(x_i - \bar{x})^2} = \frac{Cov(X,Y)}{Var(X)} = \frac{Cor(X,Y)SD(Y)}{SD(X)}$

Now $R^2 = \frac{\displaystyle\sum_{i=1}^{n}(\hat{y_i} - \bar{y})^2}{\displaystyle\sum_{i=1}^{n}(y_i - \bar{y})^2} =
\displaystyle\frac{\displaystyle\sum_{i=1}^{n}(\bar{y} + \beta_1(x_i - \bar{x}) - \bar{y})^2}{SD(Y)^2} =
\displaystyle\frac{\displaystyle\sum_{i=1}^{n}(\beta_1(x_i - \bar{x}))^2}{SD(Y)^2} = \beta_1^2\displaystyle\frac{\displaystyle\sum_{i=1}^{n}(x_i - \bar{x})^2}{SD(Y)^2} = \\
\frac{Cor(X,Y)^2SD(Y)^2}{SD(X)^2} \cdot \frac{SD(X)^2}{SD(Y)^2} = Cor(X,Y)^2$


## Applied

### Question 8

#### a
\
```{r question_8_a}
df_auto <- Auto
simple_linear_regression_model <- lm(mpg ~ horsepower, data = df_auto)

print(summary(simple_linear_regression_model))

```

##### i
Since this is simple linear regression, the t-statistic and the F-statistic are testing the same thing:
is there a relationship between the predictor and the response? Since the p-value associated with these statistics
is tiny, we reject the null hypothesis that there is no linear association between `mpg` and `horsepower`.

##### ii
This model has an $R^2$ of 0.61, indicating that 61% of the variance in the training data
is explained by the model. The estimate of the standard error is `4.9` units, relative to the mean
value of `23.4`, giving a relative error of roughly 20%.

##### iii
Looking at the sign of the coefficient, the relationship is negative.

##### iv
\
```{r question_8_a_iv}

dfs_interval <- lapply(
  c("confidence", "prediction"),
  function(interval, model) {
    df <- data.frame(predict(model, data.frame(horsepower = 98), interval = interval))
    df$interval <- interval

    df
  },
  model = simple_linear_regression_model
)

df_interval <- dplyr::bind_rows(dfs_interval)
rownames(df_interval) <- NULL

print(df_interval)

```


##### v
\
```{r question_8_v}

coefs <- coef(simple_linear_regression_model)
df_auto %>%
  ggplot2::ggplot(.) +
  ggplot2::geom_point(ggplot2::aes(x = horsepower, y = mpg)) +
  ggplot2::geom_abline(slope = coefs[[2]], intercept = coefs[[1]])

```


##### vi
\
```{r question_8_vi}
ggplot2::autoplot(simple_linear_regression_model)
```


From this plot we notice a few things:

1. Evidence of non-linearity of data - The residuals vs fitted value plot shows evidence of
a non-linear relationship, as there is a noticeable pattern to the residuals. You can also see this
in the normal QQ plot.

2. No evidence of correlation of error terms - Since each measurement is independent of the other
measurements in this data set, this is not a surprise.

3. Evidence of non-constant variance of error terms - The spread of the residuals is greater at higher values of the fitted
values, which is related to point 1 above. You can also see this in the normal QQ plot.

4. Leverage - Looking at the standardized residuals vs leverage plot, there are a number of points with high leverage, but most of them have small residuals, i.e. are not outliers. The exceptions are observations `117` and two points which lie right on top of each other, all of which have high leverage and could also be considered borderline outliers (the combination of which gives a high value of Cook's distance).

5. Outliers - There are a few points with standardized residuals with magnitude greater than 2, but most of them have low leverage
and thus do not affect the slope of the linear regression line greatly. As mentioned above, points `191` and `117` are exceptions.


### Question 9

#### a
\
```{r question_9_a}
GGally::ggpairs(df_auto %>% dplyr::select(., -name))
```

#### b
The correlations can be seen in the above plot.


#### c
\
```{r question_9_c}
multiple_linear_regression_model <- lm(mpg ~ . - name, data = df_auto)

print(summary(multiple_linear_regression_model))

```

##### i
The tiny p-value associated with the F-statistic indicates that not all of the coefficients
are equal to 0, i.e. there is a relationship between at least one of the predictors and `mpg`.

##### ii
Looking at the p-values, `displacement`, `weight`, `year`, and `origin` all appear to have a
statistically significant relationship with `mpg`.

##### iii
The coefficient for the `year` variable is positive, indicating that `mpg` has been increasing over time.


#### d
\
```{r question_9_d}
ggplot2::autoplot(multiple_linear_regression_model)
```

1. Evidence of non-linearity of data - The residuals vs fitted value plot shows evidence of
a non-linear relationship, as in the case of the earlier simple linear regression. The non-linearity
is not as severe as in the single variable case, however.

2. No evidence of correlation of error terms - Same as single variable case.

3. Evidence of non-constant variance of error terms - The spread of the residuals is greater at higher values of the fitted
values, which is related to point 1 above. You can also see this in the normal QQ plot.

4. Leverage - Looking at the standardized residuals vs leverage plot, there is one point (point 14) with very high leverage and also a relatively large residual.

5. Outliers -there are a few points with standardized residuals with magnitude greater than 2, but most of them have low leverage
and thus do not affect the slope of the linear regression line greatly. Point 14 is a borderline outlier and has high leverage.


#### e
\
```{r question_9_e}
interaction_model <- lm(mpg ~ . -name + acceleration:weight + displacement:horsepower, data = df_auto)

print(summary(interaction_model))
```
The `displacement-horsepower` interaction term is significant.  

#### f
\
```{r question_9_f}
polynomial_model <- lm(mpg ~ poly(horsepower, 2), data = df_auto)

print(summary(polynomial_model))

sqrt_model <- lm(mpg ~ I(sqrt(horsepower)), data = df_auto)

print(summary(sqrt_model))
```

In the polynomial_model, both the linear and quadratic term are statistically significant. The overall fit of
the model is superior to the simple linear model, at least using adjusted R-squared. The square root term is also significant
in the square root model, although this model does not perform as well as the quadratic model using adjusted R-squared.


### Question 10

#### a
\
```{r question_10_a}
df_carseats <- Carseats
model <- lm(Sales ~ Price + Urban + US, data = df_carseats)
print(summary(model))
```

#### b

1. Price - for a 1 unit change in price, sales go down on average by -0.05 units.
2. Urban - sales in urban populations are on average lower by -0.02 units, although this effect is non-significant.
3. US - sales in the US are on average 1.2 units higher than sales outside the US.


#### c
$\hat{y} = 13.043469 -0.054459 \cdot Price -0.021916 \cdot \mathbf 1_{Urban==Yes} + 1.200573 \cdot \mathbf 1_{US==Yes}$


#### d
\
```{r question_10_d}

model <- lm(Sales ~ Price + US, data = df_carseats)

print(summary(model))
```

This model, created by removing the insignificant `Urban` variable from the first model,
has the same $R^2$ up to 4 decimal places, higher adjusted R squared, and lower residual standard error, compared to the first model. We thus would favor this model, as it likely would perform better out of sample and is simpler.

#### e
\
```{r question_10_e}

print(confint(model))
```


#### f
\
```{r question 10_f}
ggplot2::autoplot(model)
```

The residual vs fitted plot looks good; no evidence of homoscedasticity or non-linear relationships. The QQ plot indicates that the residuals are approximately normally distributed. In terms of leverage, there is one very high leverage point but it does not have a large residual so its influence on the least squares plane is minimal. There are a few points with studentized residuals close to 3 in absolute values, indicating outliers, but they have low leverage so are unlikeley to influence the least squares plane.


### Question 11

#### a
\
```{r question_11_a}
set.seed(1)
df <- data.frame(x = rnorm(100))
df$y <- 2 * df$x + rnorm(100)

model <- lm(y ~ 0 + x, data = df)

print(summary(model))
```

The estimate is close to the true value of $\beta$.


#### b, c
\
```{r question_11_b}
model <- lm(x ~ 0 + y, data = df)

print(summary(model))

```

The same t-value is obtained in both cases.

#### d
For notational convenience, let $num = \displaystyle\sum_{i=1}^{n} y_ix_i$ and
$denum = \displaystyle\sum_{i=1}^{n} x_i^2$

Then

$t = \frac{\hat{\beta}}{SE(\hat{\beta})} =
\frac{num}{denum} /
\frac{\displaystyle\sum_{i=1}^{n} (y_i - x_i\hat{\beta})^2}{(n - 1)denum} =
\frac{num}{denum} /
\sqrt{\frac{\displaystyle\sum_{i=1}^{n} (y_i - x_i\frac{num}{denum})^2}{(n - 1)denum}} =
\frac{num}{denum} /
\sqrt{\frac{\displaystyle\sum_{i=1}^{n} y_i^2 - 2y_ix_i\frac{num}{denum} + x_i^2\frac{num^2}{denum^2}}{(n - 1)denum}} =
\sqrt{(n - 1)}num /
\sqrt{\displaystyle\sum_{i=1}^{n} (y_i^2denum - 2y_ix_inum) + num^2} =
\sqrt{(n - 1)} \displaystyle\sum_{i=1}^{n} y_ix_i/
\sqrt{\displaystyle\sum_{i=1}^{n} y_i^2\displaystyle\sum_{i=1}^{n} x_i^2 - (\displaystyle\sum_{i=1}^{n} y_ix_i )^2}$

#### e
$x_i$ and $y_i$ are symmetric in the above equation, so the result will be the same whether we
regress y on x or x on y.


#### f
\
```{r question_11_f}
y_on_x <- lm(y ~ x, data = df)
x_on_y <- lm(x ~ y, data = df)

print(summary(y_on_x)$coefficients[2, "t value"])
print(summary(x_on_y)$coefficients[2, "t value"])

```

We see that the t values are the same in this case as well.


### Question 12

#### a
When $\displaystyle\sum_{i=1}^{n} y_i^2 = \displaystyle\sum_{i=1}^{n} x_i^2$, the coefficient estimates will be the same.


#### b, c
\
```{r question_12_b_c}
set.seed(1)

df <- data.frame(x = rnorm(100))
df$y1 <- df$x + 1
df$y2 <- sample(df$x)

base_vars <- c("x", "y")
for (index in seq(2)) {
  for (pair in list(base_vars, rev(base_vars))) {
    pair[pair == "y"] <- paste0("y", index)
    formula_string <- paste(pair[[1]], "~", pair[[2]], "+ 0")
    model <- lm(as.formula(formula_string), data = df)
    print(paste0("Regression for ", pair[[1]], " on ", pair[[2]], ": "))
    print(coef(model)[[1]])
  }
}
```


We see that when the magnitude of the `x`, `y` vectors are the same, the coefficient
estimates are the same whether we regress `y` on `x` or `x` on `y`.


### Question 13
We will answer all questions in one code block here.
\
```{r question_13}

generate_data <- function(sd_eps) {
  set.seed(1)

  df <- data.frame(x = rnorm(100), eps = rnorm(100, sd = sd_eps))
  df$y <- -1 + 0.5*df$x + df$eps

  df
}

fit_model_and_plot <- function(df, sd_eps) {
  model <- lm(y ~ x, data = df)
  model_poly <- lm(y ~ poly(x, 2), data = df)
  quadratic_p_value <- summary(model_poly)$coefficients[3, 4]

  print(paste("Confidence interval for estimates using eps =", sd_eps, ":"))
  print(confint(model))
  print(paste("p-value for quadratic term using eps =", sd_eps, ":", quadratic_p_value, sep = " "))

  estimated_values <- coef(model)
  true_values <- c(-1, 0.5)

  df_coefficients <- data.frame(
    intercept = c(estimated_values[[1]], true_values[[1]]),
    slope = c(estimated_values[[2]], true_values[[2]]),
    label = c("Estimated Regression Line", "Population Regression Line")
  )

  ggplot2::ggplot(df) +
    ggplot2::geom_point(ggplot2::aes(x = x, y = y)) +
    ggplot2::geom_abline(ggplot2::aes(intercept = intercept, slope = slope, color = label), data = df_coefficients) +
    ggplot2::labs(title = paste("Regression lines and data points using eps == ", sd_eps, sep = " ")) +
    ggplot2::lims(y = c(-4, 2))
}

generate_data_fit_model_plot <- function(sd_eps) {
  df <- generate_data(sd_eps)
  fit_model_and_plot(df, sd_eps)
}

plots <- lapply(c(0.01, 0.25, 1), generate_data_fit_model_plot)

for (plot in plots) {
  print(plot)
}

```

The length of `y` is 100. $\beta_0$ is -1, and $\beta_1$ is 0.5. This does not change as we vary the
error term. As the error term variance increases, the estimated regression line diverges from the true regression line and the confidence intervals around the coefficients becomes larger.
The polynomial term is never significant regardless of the value of the error term variance.



### Question 14

#### a
\
```{r question_14_a}
set.seed(1)

df <- data.frame(x1 = runif(100))
df$x2 <- 0.5  * df$x1 + rnorm(100) / 10
df$y <- 2 + 2 * df$x1 + 0.3 * df$x2 + rnorm(100)

```


The linear model is $y = 2 + 2x_1 + 0.3x_2 + \epsilon$. The population regression coefficients
are $\beta_0 = 2, \beta_1 = 2, \beta_2 = 0.3$


#### b
\
```{r question_14_b}
GGally::ggpairs(df, columns = c("x1", "x2"))
```

From the above plots we see the correlation is 0.818.


#### c
\
```{r question_14_c}
model <- lm(y ~ ., data = df)

print(summary(model))
```

We can reject the null hypothesis $H_0 : \beta_1 = 0$, but not the null hypothesis $H_0 : \beta_2 = 0$.


#### d
\
```{r question_14_d}
model <- lm(y ~ x1, data = df)

print(summary(model))
```
Again, we can reject the null hypothesis $H_0 : \beta_1 = 0$. The p-value is lower in the univariate case.

#### e
\
```{r question_14_e}
model <- lm(y ~ x2, data = df)

print(summary(model))
```
This time we can reject the null hypothesis $H_0 : \beta_2 = 0$.


#### f

These results do not contradict one another; becuause of the high correlation between
the two variables, the variance of the estimates when they are both included in the model is quite high.
In other words, there are many possible pairs of estimates that fit the data about equally well, so we can't be very certain that a given estimate is close to the true population value.


#### g
\
```{r question 14_g}
df <- rbind(df, list(x1 = 0.1, x2 = 0.8, y = 6))

model_both <- lm(y ~ ., data = df)

print(summary(model_both))

ggplot2::autoplot(model_both)

model_x1 <- lm(y ~ x1, data = df)

print(summary(model_x1))

ggplot2::autoplot(model_x1)

model_x2 <- lm(y ~ x2, data = df)

print(summary(model_x2))


```

The statistically significant term changes from `x1` to `x2` in the model including both terms. This point has high leverage in all 3 models, and is the highest leverage point in models 1 and 3. In model 2, it could be considered an outlier given the high value of the standardized residual.


### Question 15

#### a
\
```{r question_15_a}

response <- "crim"
predictors <- setdiff(colnames(df_boston), response)

univariate_coefficients <- rep(0, length(predictors)) %>%
  setNames(., predictors)
for (predictor in predictors) {
  formula_string <- paste(response, "~", predictor, sep = " ")
  model <- lm(as.formula(formula_string), df_boston)
  univariate_coefficients[[predictor]] <- coef(model)[predictor]
  print(summary(model))
}

```

We see that everything except for `chas` is statistically significant in the univariate models.

#### b
\
```{r question_15_b}

model <- lm(crim ~ ., data = df_boston)

print(summary(model))

multivariate_coefficients <- coef(model)[predictors]
```

`zn`, `dis`, `rad`, `black`, and `medv` are statistically significant.


#### c
\
```{r question_15_c}
df_coefficients <- data.frame(
  univariate = univariate_coefficients,
  multivariate = multivariate_coefficients,
  variable = names(univariate_coefficients)
)

ggplot2::ggplot(df_coefficients) +
  ggplot2::geom_point(ggplot2::aes(x = multivariate, y = univariate, color = variable))

```

`nox` has a very different value between the univariate and multivariate analyses.



#### d
\
```{r question_15_d}

for (predictor in setdiff(predictors, "chas")) {
  poly_formula_string <- paste0(response, " ~ poly(", predictor, ", 3)")
  linear_formula_string <- paste(response, "~", predictor, sep = " ")
  model_poly <- lm(as.formula(poly_formula_string), data = df_boston)
  model_linear <- lm(as.formula(linear_formula_string), data = df_boston)

  print(anova(model_linear, model_poly))
}

```

Looking at the anova tables, we see evidence of non-linear relationships for every variable except for
`black`. `chas` was excluded because there are only two unique values (dummy variable).
