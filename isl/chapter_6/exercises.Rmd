---
title: ISLR Chapter 6 Exercises
date: 2023-07-01
linkcolor: blue
toccolor: blue
output:
  md_document:
    toc: true
    toc_depth: 3
  html_document:
    toc: true
    toc_depth: 3
  pdf_document:
    toc: true
    toc_depth: 3
---

```{r setup, echo = FALSE}
library(knitr)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r load_libraries}
library(ISLR)
library(tools)
library(ggplot2)
library(leaps)
library(glmnet)
library(pls)
library(dplyr)
library(R6)
```


## Conceptual

### Question 1

#### a
Best subset will have the smallest training RSS, as for a given number of variables in the model, the models considered by
forward / backward stepwise selection will be a subset of the models chosen by best subset selection. Best subset therefore looks at every model that forward / backward stepwise selection looks at, in addition to other models not examined by the other two procedures. Since it is possible that some of these other models have lower training RSS, the model chosen by best subset will have RSS equal to or lower than the models chosen by forward/backward stepwise selection.

#### b
This is impossible to answer; best subset might have picked a model that is overfit to the data, since it has more candidate models to choose from, in which case the other two methods would have better test RSS, or it might have picked a model that actually describes the response-predictor relationship better than the other two methods, in which case it would have better test RSS than the other methods.

#### c

##### i
True

##### ii
True

##### iii
False

##### iv
False

##### v
False

### Question 2

#### a
`iii` is correct because lasso gives biased estimates of the regression coefficients, but they
have lower variance than the OLS estimates. Depending on the balances of these two competing factors,
the prediction accuracy could be worse or better.  


#### b
`iii`, for the same reasons as a.


#### c
`ii` as non-linear methods are more flexible and hence have lower bias but higher variance.


### Question 3

#### a
`iv`. The training error will always decrease because as we increase `s`, we decrease the effect of the inequality constraint
so that the minimization of the sum of squared errors dominates the objective function. As `s` approaches `infinity`, there is no inequality constraint, so the objective purely minimizes the sum of squared errors, which is the RSS.

#### b
`ii`. The test error will initially decrease, as the decrease in bias will outweigh the increase in variance. As we continue to
increase `s`, however, the increase in variance will outweigh the decrease in bias and the test error will start to increase.

#### c
`iii`. Variance always increases as  model flexibility increases, so increasing `s` will always increase variance.

#### d
`iv`. Bias always decreases as model flexibility increases, so increasing `s` will always decrease bias.


#### e
`v`. The irreducible error is constant, as this is a property of the test point rather than the trained model.

### Question 4

#### a
`iii`. The training error will always increase because as we increase $\lambda$, we increase the effect of the inequality constraint
so that the minimization of the sum of squared errors plays less of a role in the objective function. As $\lambda$ approaches `infinity`, all the coefficients except the intercept approach 0, and assuming the predictors have been centered to have mean 0 the train RSS will simply be the scaled variance of the target variable, $(\bar{y} - y_i)^2$

#### b
`ii`. The test error will initially decrease, as the decrease in variance will outweigh the increase in bias. As we continue to
increase $\lambda$, however, the increase in bias will outweigh the decrease in variance and the test error will start to increase.

#### c
`iv`. Variance always decreases as model flexibility decreases, so increasing $\lambda$ will always decrease variance.

#### d
`iii`. Bias always increases as model flexibility decreases, so increasing $\lambda$ will always increase bias.

#### e
`v`. The irreducible error is constant, as this is a property of the test point rather than the trained model.

### Question 5

#### a
$\displaystyle\min_{\beta_1,\beta_2} \displaystyle\sum_{i=1}^{2}(\beta_1x_{i1} + \beta_2x_{i2} - y_i)^2 + \displaystyle\sum_{j=1}^{2}\beta_j^2$


#### b
Differentiate with respect to $\beta_1$ and set equal to zero to solve for beta:

$2\displaystyle\sum_{i=1}^{2}x_{i1}(\beta_1x_{i1} + \beta_2x_{i2} - y_i) + 2\lambda\beta_1 = 0$

$\displaystyle\sum_{i=1}^{2}(x_{i1}^2\beta_1 + x_{i1}^2\beta_2 - y_i) + \lambda\beta_1 = 0$

$\displaystyle\sum_{i=1}^{2}x_{i1}^2\beta_1 + \lambda\beta_1 = \displaystyle\sum_{i=1}^{2}-x_{i1}^2\beta_2 + y_ix_i$


$\beta_1 = \frac{\displaystyle\sum_{i=1}^{2}-x_{i1}^2\beta_2 + y_ix_i}{\lambda + \displaystyle\sum_{i=1}^{2}x_{i1}^2}$


Similarly we find that

$\beta_2 = \frac{\displaystyle\sum_{i=1}^{2}-x_{i1}^2\beta_1 + y_ix_i}{\lambda + \displaystyle\sum_{i=1}^{2}x_{i1}^2}$


We can rewrite this as

1) $\beta_1 = C + D\beta_2$

2) $\beta_2 = C + D\beta_1$


Substituting 1 into 2, we get

$\beta_2 = C + D(C + D\beta_2)$

$\beta_2 = C + DC + D^2\beta_2$

$\beta_2(1 - D^2) = C + DC$

And as long as $D^2 \neq 1$, or $\lambda > 0$, we can solve for $\beta_2$ uniquely:
$\beta_2 = \frac{C + DC}{1 - D^2}$.

We can go through the same exercise with $\beta_1$ and arrive at the same answer.
Therefore $\beta_1 = \beta_2$  


#### c
$\displaystyle\min_{\beta_1,\beta_2} \displaystyle\sum_{i=1}^{2}(\beta_1x_{i1} + \beta_2x_{i2} - y_i)^2 + \lambda\displaystyle\sum_{j=1}^{2}|\beta_j|$


#### d
This part is easier if we rewrite the above in its alternative formulation,

$\displaystyle\min_{\beta_1,\beta_2} \displaystyle\sum_{i=1}^{2}(\beta_1x_{i1} + \beta_2x_{i2} - y_i)^2\,\,s.t.\,\displaystyle\sum_{j=1}^{2}|\beta_j| \le t$


The level curves of the function we are minimizing are given by

$(\beta_1x_{11} + \beta_2x_{11} - y_1)^2  + (\beta_1x_{21} + \beta_2x_{21} - y_2)^2 = C$

$2(\beta_1x_{11} + \beta_2x_{11} - y_1)^2 = C$

$(\beta_1 + \beta_2)x_{11} - y_1 = \displaystyle\pm \sqrt{\frac{C}{2}}$

$\beta_1 + \beta_2 = \frac{\displaystyle\pm \sqrt{\frac{C}{2}} + y_1}{x_{11}}$



Which implies that the level curves are lines in the $\beta_1, \beta_2$ space. If $\lambda$ is non-zero, this implies that
the inequality constraint is binding, or that the inequality is in fact an equality. This means that the solution must lie on one of the edges of the diamond represented by $\displaystyle\sum_{j=1}^{2}|\beta_j| \le t$. Since these edges are also straight lines, they touch one of the level curves at infinitely many points.

### Question 6

First I will derive a more general form of the ridge regression estimates when the design matrix is
orthogonal. I assume that the predictors have been centered to have mean 0 as well, so that the estimate for
$\beta_0$ is equal to $\hat{y}$ for the OLS estimates, ridge estimates, and lass estimates. The vector
$\beta$ that follows is then the vector of coefficients with the first element removed.

**Least squares loss function:**

$\displaystyle\min_{\beta} (y - X\beta)^T(y - X\beta)$

$\displaystyle\min_{\beta} (y^T - \beta^TX^T)(y - X\beta)$


$\displaystyle\min_{\beta} y^Ty - y^TX\beta - \beta^TX^Ty + \beta^TX^TX\beta$

$\displaystyle\min_{\beta} y^Ty - 2\beta^TX^Ty + \beta^TX^TX\beta$



Differentiate with respect to $\beta$ and set equal to zero:

$-2X^Ty + 2X^TX\beta = 0$

$\beta = (X^TX)^{-1}X^Ty$

Since X is an orthogonal matrix, $X^TX = I$ and $(X^TX)^{-1} = I$

Then $\beta = X^Ty$

**Ridge regression loss function:**

$\displaystyle\min_{\beta} (y - X\beta)^T(y - X\beta) + \lambda \beta^T\beta$

$\displaystyle\min_{\beta} (y^T - \beta^TX^T)(y - X\beta) + \lambda \beta^T\beta$


$\displaystyle\min_{\beta} y^Ty - y^TX\beta - \beta^TX^Ty + \beta^TX^TX\beta + \lambda \beta^T\beta$

$\displaystyle\min_{\beta} y^Ty - 2\beta^TX^Ty + \beta^TX^TX\beta + \lambda \beta^T\beta$

Differentiate with respect to $\beta$ and set equal to zero:

$-2X^Ty + 2X^TX\beta + 2\lambda\beta = 0$

$\beta = (X^TX + \lambda I)^{-1}X^Ty$

Since X is an orthogonal matrix, $X^TX + \lambda I = (1 + \lambda)I$ and $(X^TX + \lambda I)^{-1} = \frac{1}{1 + \lambda}I$

Then $\beta = \frac{X^Ty}{1 + \lambda} = \frac{\beta_{OLS}}{1 + \lambda}$

**Lasso loss function :**

$\displaystyle\min_{\beta} (y - X\beta)^T(y - X\beta) + \lambda ||\beta||_1$

$\displaystyle\min_{\beta} (y^T - \beta^TX^T)(y - X\beta) + \lambda ||\beta||_1$


$\displaystyle\min_{\beta} y^Ty - y^TX\beta - \beta^TX^Ty + \beta^TX^TX\beta + \lambda ||\beta||_1$

$\displaystyle\min_{\beta} y^Ty - 2\beta^TX^Ty + \beta^TX^TX\beta + \lambda ||\beta||_1$

$\displaystyle\min_{\beta} y^Ty - 2\beta^TX^Ty + \beta^T\beta + \lambda ||\beta||_1$

$\displaystyle\min_{\beta} y^Ty - 2\beta^T\beta_{ols} + \beta^T\beta + \lambda ||\beta||_1$

Take an element `i` of the gradient and set equal to 0. We refer to the coefficient corresponding to this as $\beta$ now, which is a scalar rather than a vector.

There are two cases:

1. $\beta_{ols} \ge 0$, in which case $\beta \ge 0$ so as to minimize the loss function.

    Then
    $-2\beta_{ols} + 2\beta + \lambda  = 0$

    $\beta = \beta_{ols} - \frac{\lambda}{2}$

    Since we assumed that $\beta \ge 0$, we take only the positive part of this equation:

    $\beta = (\beta_{ols} - \frac{\lambda}{2})^+$

    $\beta = (sign(\beta_{ols})|\beta_{ols}| - \frac{\lambda}{2})^+$


2. $\beta_{ols} \le 0$, in which case $\beta \le 0$ so as to minimize the loss function.

    Then
    $-2\beta_{ols} + 2\beta - \lambda  = 0$

    $\beta = \beta_{ols} + \frac{\lambda}{2}$

    Since we assumed that $\beta \le 0$, we take only the negative part of this equation:

    $\beta = (\beta_{ols} + \frac{\lambda}{2})^-$

    $\beta = sign(|\beta_{ols})(|\beta_{ols}| - \frac{\lambda}{2})^+$


We get the same equation for $\beta$ in both cases.

I answer this using the general form of the ridge regression / lasso estimates in the case of a centered variable with magnitude 1.

#### a
\
```{r question_6_a}
generate_response <- function(df, population_coefficient) {
  df$y <- 10 + population_coefficient * df$x + df$eps
  df
}

compare_theoretical_to_simulation <- function(df, betas, method = c("ridge", "lasso")) {

  lambda <- 0.5

  ols_model <- lm(y ~ x, data = df)
  ols_coef <- coef(ols_model)[[2]]

  method <- match.arg(method)
  regularizer <- switch(
    method,
    ridge = function(beta) beta ^ 2,
    lasso = function(beta) abs(beta)
  )

  scaling_factor <- switch(
    method,
    ridge = function(lambda, beta_ols) beta_ols / (1 + lambda),
    lasso = function(lambda, beta_ols) {
      if (abs(beta_ols) >= lambda / 2) {
        sign(beta_ols) * (abs(beta_ols) - lambda / 2)
      } else {
        0
      }
    }
  )

  loss <- sapply(
    betas,
    function(beta, df, lambda, regularizer) {
      sum((df$y - mean(df$y) - beta * df$x) ^ 2) + lambda * regularizer(beta)
    },
    df = df,
    lambda = lambda,
    regularizer = regularizer
  )

  df_loss <- data.frame(beta = betas, loss = loss)
  df_vertical_lines <- data.frame(
    estimates = c(ols_coef, scaling_factor(lambda, ols_coef)),
    method = c("ols", method)
  )

  plot_results(df_loss, df_vertical_lines)

}

plot_results <- function(df_loss, df_vertical_lines) {
  ggplot2::ggplot(data = df_loss) +
    ggplot2::geom_point(ggplot2::aes(x = beta, y = loss)) +
    ggplot2::geom_vline(
      data = df_vertical_lines,
      ggplot2::aes(xintercept = estimates,
        color = tools::toTitleCase(method))
      ) +
    ggplot2::labs(x = expression(beta), y = "Loss", color = "Method")
}

nrows <- 10000

set.seed(1)
df <- data.frame(x = rnorm(nrows), eps = rnorm(nrows, sd = 0.001))
df$x <- df$x / sqrt(sum(df$x^2))
df <- generate_response(df, population_coefficient = 1)

betas <- seq(-1, 3, length = 10000)

plot_ridge <- compare_theoretical_to_simulation(df, betas, method = "ridge")
plot_ridge <- plot_ridge +
  ggplot2::ggtitle(expression(Loss~versus~beta~ridge~regression))

plot_ridge

```

We see that the theoretical value of $\beta_{ridge}$ aligns well with the minimum of the loss in the simulation.

#### b

We will consider three cases here:

1. $\beta_{ols} < 0$ and $\beta_{ols} < -\frac{\lambda}{2} 0$
2. $\beta_{ols} > 0$ and $\beta_{ols} > \frac{\lambda}{2} 0$
3. $\beta_{ols} > 0$ and $\beta_{ols} < \frac{\lambda}{2} 0$

```{r question_6_b}

param_list <- list(
  lasso_1 = list(
    betas = seq(-1, 3, length = 10000),
    population_coefficient = 1,
    title = expression(Loss~versus~beta~lasso~with~beta[ols]>0~and~beta[ols]>lambda/2)
  ),
  lasso_2 = list(
    betas = seq(-3, 1, length = 10000),
    population_coefficient = -1,
    title = expression(Loss~versus~beta~lasso~with~beta[ols]<0~and~paste("|", beta[ols], "|")>lambda/2)
  ),
  lasso_3 = list(
    betas = seq(-0.15, 0.15, length = 10000),
    population_coefficient = 0.01,
    title = expression(Loss~versus~beta~lasso~with~beta[ols]>0~and~beta[ols]<lambda/2)
  )
)

for (params in param_list) {
  df_lasso <- generate_response(df, params$population_coefficient)
  plot <- compare_theoretical_to_simulation(df_lasso, params$beta, method = "lasso")
  plot <- plot + ggplot2::labs(title = params$title)
  print(plot)
}

```

### Question 7

#### a
Let $y_i = \beta_0 + \beta^Tx_i + \epsilon_i$, where $\beta$ is the vector of coefficients without the intercept.

$p(y | X) = \displaystyle\prod_{i = 1}^{n} \frac{1}{\sqrt(2\pi \sigma^2)} exp^{\frac{-(y_i - \beta_0 - \beta^Tx_i)^2}{2\sigma^2}}$

#### b

$p(\beta | X) = \frac{\displaystyle\prod_{i = 1}^{n} \frac{1}{\sqrt(2\pi \sigma^2)} exp^{\frac{-(y_i - \beta_0 - \beta^Tx_i)^2}{2\sigma^2}} \displaystyle\prod_{j = 1}^{p}\frac{1}{2b}exp^{-\frac{|\beta_p|}{b}}}{p(X)}$


#### c

To find the mode of this posterior distribution, we maximize with respect to $\beta$. Since the `argmax` of a function is equal to the `argmax` of the log of the function, we take the log of the posterior and discard constant terms, which are the constants multiplying the exponential functions and the denominator.

$\max_{\beta}\displaystyle\sum_{i = 1}^{n}\frac{-(y_i - \beta_0 - \beta^Tx_i)^2}{2\sigma^2} +
\displaystyle\sum_{j = 1}^{p} -\frac{|\beta_p|}{b}$

$\min_{\beta}\displaystyle\sum_{i = 1}^{n}\frac{(y_i - \beta_0 - \beta^Tx_i)^2}{2\sigma^2} +
\displaystyle\sum_{j = 1}^{p} \frac{|\beta_p|}{b}$

$\min_{\beta}\displaystyle\sum_{i = 1}^{n}(y_i - \beta_0 - \beta^Tx_i)^2 +
\displaystyle\sum_{j = 1}^{p} \frac{2\sigma^2|\beta_p|}{b}$

Which is equivalent to the lasso formulation if $\lambda = \frac{2\sigma^2}{b}$

#### d

$p(\beta | X) = \frac{\displaystyle\prod_{i = 1}^{n} \frac{1}{\sqrt{2\pi \sigma^2}} exp^{\frac{-(y_i - \beta_0 - \beta^Tx_i)^2}{2\sigma^2}} \displaystyle\prod_{j = 1}^{p}\frac{1}{\sqrt{2\pi c^2}}exp^{-\frac{\beta_p^2}{2c}}}{p(X)}$


#### e
We follow the same logic as part c) to get
$\max_{\beta}\displaystyle\sum_{i = 1}^{n}\frac{-(y_i - \beta_0 - \beta^Tx_i)^2}{2\sigma^2} +
\displaystyle\sum_{j = 1}^{p} -\frac{\beta_p^2}{2c}$

$\min_{\beta}\displaystyle\sum_{i = 1}^{n}\frac{(y_i - \beta_0 - \beta^Tx_i)^2}{2\sigma^2} +
\displaystyle\sum_{j = 1}^{p} \frac{\beta_p^2}{2c}$

$\min_{\beta}\displaystyle\sum_{i = 1}^{n}(y_i - \beta_0 - \beta^Tx_i)^2 +
\displaystyle\sum_{j = 1}^{p} \frac{2\sigma^2\beta_p^2}{2c}$

Which is equivalent to the ridge formulation if $\lambda = \frac{\sigma^2}{c}$

## Applied


### Question 8

#### a-b
\
```{r question_8_a_b}
set.seed(1)
df <- data.frame(x_1 = rnorm(100), eps = rnorm(100))

for (power in seq(2, 10)) {
  df[[paste("x", power, sep = "_")]] <- df$x_1 ^ power
}
df$y <- 1 + 2 * df$x_1 + 3 * df$x_2 + 4 * df$x_3 + df$eps
```

#### c-d
\
```{r question_9_c_d}

metrics <- c("bic", "cp", "adjr2")
methods <- c("exhaustive", "forward", "backward")

for (method in methods) {
  models <- leaps::regsubsets(y ~ . - eps, data = df, nvmax = ncol(df) - 2, method = method)
  models_summary <- summary(models)
  for (metric in metrics) {
    values <- models_summary[[metric]]
    if (identical(metric, "adjr2")) {
      ids <- which.max(values)
      min_max_function <- max
    } else {
      idx <- which.min(values)
      min_max_function <- min
    }
    coefs <- coef(models, idx)
    cat(
      paste(
        "Coefficients for best", method, "model using", metric, ": \n",
        paste(paste(names(coefs), round(coefs, 2), sep = ":"), collapse = ", "),
        "\n"
      )
    )

    df_for_plot <- data.frame(value = values, n_vars = seq(length(values)))

    df_min_max <- df_for_plot %>%
      dplyr::filter(., value == min_max_function(value))

    plt <- ggplot2::ggplot(data = df_for_plot) +
      ggplot2::geom_line(ggplot2::aes(x = n_vars, y = value)) +
      ggplot2::geom_point(data = df_min_max, ggplot2::aes(x = n_vars, y = value), color = "red") +
      ggplot2::labs(y = metric) +
      ggplot2::scale_x_continuous(breaks = seq(length(values)), minor_breaks = NULL)

    print(plt)
  }
}

```
Using the BIC criterion we choose the correct model. The other two methods, `Cp` and `adjr2`, pick a model with the correct three variables in addition to $x^5$ or $x^9$


#### e
\
```{r question_8_e}
x_mat <- as.matrix(df[paste("x", seq(10), sep = "_")])
y <- df$y

set.seed(1)
lasso_model <- glmnet::cv.glmnet(x_mat, y)

# Use lambda1.se here
print(coef(lasso_model))
```

We see that the coefficient estimates for variables that are in the true equation are close to the
actual values, but differ somewhat. The estimates for the variables that are not in the true regression model
are either 0 or close to 0.


#### f
\
```{r question_8_f}
df$y <- 1 + 6 * df$x_7 + df$eps

exhaustive_model <- leaps::regsubsets(y ~ . - eps, data = df, nvmax = ncol(df) - 2)
for (metric in metrics) {
  values <- summary(exhaustive_model)[[metric]]
  if (identical(metric, "adjr2")) {
    ids <- which.max(values)
  } else {
    idx <- which.min(values)
  }
  coefs <- coef(exhaustive_model, idx)
  cat(
    "Coefs for best exhaustive model using",
    metric,
    ":\n",
    paste(paste(names(coefs), round(coefs, 2), sep = ": "), collapse = ", "),
    "\n"
  )

}

x_mat <- as.matrix(df[paste("x", seq(10), sep = "_")])
y <- df$y

set.seed(1)
lasso_model <- glmnet::cv.glmnet(x_mat, y)

print(coef(lasso_model))
```

Using BIC as the metric for best subset yields pretty much the correct model, whereas using `adjr2` and `cp` both yield a model that has chosen $x^2$ in addition to $x^7$, albeit with a small coefficient. Lasso zeroes out all coefficients except for $x^7$, but the intercept is off substantially.


### Question 9

#### a-g
\
```{r question_9_a_g}
set.seed(1)
df_college <- ISLR::College

nrows <- nrow(df_college)
train_idx <- sample(nrows, nrows %/% 2)

df_train <- df_college[train_idx, ]
df_test <- df_college[-train_idx, ]

calculate_r_squared <- function(y_true, y_pred) {
  mean_y <- mean(y_true)
  1 - sum((y_pred - y_true) ^ 2) / sum((y_true - mean_y) ^ 2)
}

generate_cross_val_index <- function(df, k, seed = 1) {
  set.seed(seed)
  fold_idx <- rep(seq(k), length = nrow(df))
  sample(fold_idx)
}

ols_model <- R6Class(
  "ols_model",
  public = list(
    model = NULL,
    cv_r2 = NULL,
    initialize = function() {
    },
    cross_validate = function(data, dep_var, fold_idx) {

      folds <- split(data, fold_idx)
      cv_results <- rep(0, length(folds))
      model_formula <- as.formula(paste(dep_var, "~ ."))
      for (fold_idx in seq_along(folds)) {
        df_train <- do.call(rbind, folds[-fold_idx])
        model <- lm(model_formula, data = df_train)
        preds <- predict(model, folds[[fold_idx]])
        cv_results[[fold_idx]] <- calculate_r_squared(folds[[fold_idx]][[dep_var]], preds)
      }
      self$cv_r2 <- data.frame(
        model_type = "ols",
        parameter = NA,
        parameter_value = NA,
        r2 = mean(cv_results)
      )
      self
    },
    fit = function(data, dep_var) {
      model_formula <- as.formula(paste(dep_var, "~ ."))
      self$model <- lm(model_formula, data)
      self
    },
    fit_best_model = function(data, dep_var, fold_idx) {
      self$cross_validate(data, dep_var, fold_idx)
      self$fit(data, dep_var)
      self
    },
    predict = function(data) {
      predict(self$model, data)
    }
  )
)

dimensionality_reduction_model <- R6Class(
  "dimensionality_reduction_model",
  public = list(
    method_name = NULL,
    method = NULL,
    cv_r2 = NULL,
    best_ncomp = NULL,
    model = NULL,
    initialize = function(method_name) {
      stopifnot(method_name %in% c("plsr", "pcr"))
      self$method_name <- method_name
      self$method <- switch(method_name, plsr = pls::plsr, pcr = pls::pcr)
    },
    cross_validate = function(data, dep_var, fold_idx) {
      n_comps <- length(setdiff(colnames(data), dep_var))
      folds <- split(data, fold_idx)
      model_formula <- as.formula(paste(dep_var, "~ ."))
      cv_results <- matrix(0, length(folds), n_comps)
      for (fold_idx in seq_along(folds)) {
        df_train <- do.call(rbind, folds[-fold_idx])
        model <- self$method(model_formula, data = df_train)
        for (ncomp in seq(n_comps)) {
          preds <- predict(model, folds[[fold_idx]], ncomp = ncomp)
          cv_results[fold_idx, ncomp] <- calculate_r_squared(folds[[fold_idx]][[dep_var]], preds)
        }
      }
      cv_results <- apply(cv_results, 2, mean)

      self$cv_r2 <- data.frame(
        model_type = self$method_name,
        parameter = "ncomp",
        parameter_value = seq_along(cv_results),
        r2 = cv_results
      )
      self$best_ncomp <- which.max(cv_results)
      self
    },
    fit = function(data, dep_var) {
      model_formula <- as.formula(paste(dep_var, "~ ."))
      self$model <- self$method(model_formula, data = data)
      self
    },
    fit_best_model = function(data, dep_var, fold_idx) {
      self$cross_validate(data, dep_var, fold_idx)
      self$fit(data, dep_var)
      self
    },
    predict = function(data) {
      predict(self$model, data, ncomp = self$best_ncomp)
    }
  )
)

glmnet_model <- R6Class(
  "glmnet_model",
  public = list(
    cv_r2 = NULL,
    alpha = NULL,
    model = NULL,
    best_lambda = NULL,
    initialize = function(alpha) {
      self$alpha <- alpha
    },
    cross_validate = function(data, dep_var, fold_idx) {
      model_formula <- as.formula(paste(dep_var, "~ ."))
      x <- model.matrix(model_formula, data)
      y <- data[[dep_var]]
      y_mean <- mean(y)

      cv_model <- cv.glmnet(x, y, alpha = self$alpha, foldid = fold_idx, keep = TRUE)

      self$model <- cv_model
      self$best_lambda <- cv_model$lambda.min
      df_predictions <- as.data.frame(cv_model$fit.preval)
      lambda_values <- cv_model$lambda
      df_predictions$foldid <- cv_model$foldid
      df_predictions$y <- y

      unique_folds <- unique(df_predictions$foldid)
      cv_results <- matrix(0, length(unique_folds), length(lambda_values))

      for (fold_idx in unique_folds) {
        df_predictions_sub <- df_predictions[df_predictions$foldid == fold_idx, ]
        for (col_idx in seq_along(lambda_values)) {
          cv_results[fold_idx, col_idx] <- calculate_r_squared(
            df_predictions_sub$y, df_predictions_sub[, col_idx]
          )
        }
      }
      cv_results <- apply(cv_results, 2, mean)

      self$cv_r2 <- data.frame(
        model_type = "glmnet_model",
        parameter = "lambda",
        parameter_value = lambda_values,
        r2 = cv_results
      )

      self
    },
    fit_best_model = function(data, dep_var, fold_idx) {
      self$cross_validate(data, dep_var, fold_idx)

      self
    },
    predict = function(data) {
      data <- model.matrix(~ . , data = data)
      predict(self$model, data, lambda = self$best_lambda)
    }
  )
)

subsets_model <- R6Class(
  "subsets_model",
  public = list(
    method_name = NULL,
    method = NULL,
    cv_r2 = NULL,
    best_nvar = NULL,
    model = NULL,
    initialize = function(method_name) {
      stopifnot(method_name %in% c("best_subsets", "forward", "backward"))
      self$method_name <- method_name
      self$method <- switch(
        method_name,
        best_subsets = function(...) leaps::regsubsets(..., method = "exhaustive"),
        forward = function(...) leaps::regsubsets(..., method = "forward"),
        backward = function(...) leaps::regsubsets(..., method = "backward")
      )

    },
    cross_validate = function(data, dep_var, fold_idx) {
      n_vars <- length(setdiff(colnames(data), dep_var))
      folds <- split(data, fold_idx)
      model_formula <- as.formula(paste(dep_var, "~ ."))
      cv_results <- matrix(0, length(folds), n_vars)
      for (fold_idx in seq_along(folds)) {
        df_train <- do.call(rbind, folds[-fold_idx])
        model <- self$method(model_formula, data = df_train, nvmax = n_vars)
        for (nvar in seq(n_vars)) {
          x_test <- model.matrix(~ ., folds[[fold_idx]])
          coefs <- coef(model, id = nvar)
          x_test <- x_test[, names(coefs)]
          preds <- x_test %*% coefs
          cv_results[fold_idx, nvar] <- calculate_r_squared(folds[[fold_idx]][[dep_var]], preds)
        }
      }
      cv_results <- apply(cv_results, 2, mean)
      self$cv_r2 <- data.frame(
        model_type = self$method_name,
        parameter = "nvars",
        parameter_value = seq_along(cv_results),
        r2 = cv_results
      )
      self$best_nvar <- which.max(cv_results)

      self
    },
    fit = function(data, dep_var) {
      model_formula <- as.formula(paste(dep_var, "~ ."))
      self$model <- self$method(model_formula, data = data, nvmax = ncol(data) - 1)

      self
    },
    fit_best_model = function(data, dep_var, fold_idx) {
      self$cross_validate(data, dep_var, fold_idx)
      self$fit(data, dep_var)

      self
    },
    predict = function(data) {
      data <- model.matrix(~ ., data)
      coefs <- coef(self$model, id = self$best_nvar)
      data <- data[, names(coefs)]

      data %*% coefs
    }
  )
)

models <- list(
  mod_ols = ols_model$new(),
  mod_pcr = dimensionality_reduction_model$new(method = "pcr"),
  mod_plsr = dimensionality_reduction_model$new(method = "plsr"),
  mod_lasso = glmnet_model$new(alpha = 1),
  mod_ridge = glmnet_model$new(alpha = 0),
  best_subsets_model = subsets_model$new(method = "best_subsets"),
  forward_stepwise_model = subsets_model$new(method = "forward"),
  backward_stepwise_model = subsets_model$new(method = "backward")
)

cross_val_indices <- generate_cross_val_index(df_train, 5, 1)

# Cross-validate and fit models
lapply(models, function(model) model$fit_best_model(df_train, "Apps",  cross_val_indices))

# Extract r-squared
cv_r2 <- dplyr::bind_rows(
  lapply(models, function(model) model$cv_r2)
)

best_models <- cv_r2[cv_r2$r2 == max(cv_r2$r2), ]

print(best_models)

df_test_without_y <- df_test %>%
  dplyr::select(., -Apps)

test_r_2 <- lapply(
  models,
  function(model, df, y) {
    preds <- model$predict(df_test_without_y)
    calculate_r_squared(y, preds)
  },
  df = df_test_without_y,
  y = df_test$Apps
)

print(test_r_2)

```


The best model through cross-validation is chosen by backward stepwise regression.
Everything has similar $r^2$ on the test set.



### Question 10

#### a-b
\
```{r question_10_a}
set.seed(1)
nrows <- 1000
ncols <- 20

X <- matrix(rnorm(nrows * ncols), nrows, ncols)
beta <- sample(5, ncols, replace = TRUE)
# set 10 of the coefficients equal to 0
idx_0 <- sample(ncols, 10)
beta[idx_0] <- 0

eps <- rnorm(nrows)

y <- X %*% matrix(beta, ncols, 1) + eps

df <- data.frame(X)
df$y <- y

train_idx <- sample(nrows, 100)
df_train <- df[train_idx, ]
df_test <- df[-train_idx, ]

```


#### c - e
```{r question_10_c_e}
exhaustive_model <- leaps::regsubsets(y ~ ., data = df_train, nvmax = ncols)

mse <- list(
  train = rep(0, ncols),
  test = rep(0, ncols)
)

x_train <- model.matrix(y ~., df_train)
x_test <- model.matrix(y ~ ., df_test)

data_list <- list(
  train = list(
    x = x_train,
    y = df_train$y
  ),
   test = list(
     x = x_test,
     y = df_test$y
   )
)

for (i in seq(ncols)) {
  coefs <- coef(exhaustive_model, i)
  for (train_test_split in names(data_list)) {
    x_sub <- data_list[[train_test_split]]$x[, names(coefs)]
    preds <- x_sub %*% coefs

    mse[[train_test_split]][[i]] <- mean((preds  - data_list[[train_test_split]]$y) ^ 2)
  }

}

df_for_plot <- data.frame(
  mse = c(mse$train, mse$test),
  dataset = c(rep("train", ncols), rep("test", ncols)),
  n_vars = rep(seq(20), 2)
)

ggplot2::ggplot(df_for_plot) +
  ggplot2::geom_line(ggplot2::aes(x = n_vars, y = mse, color = dataset))

min_test_error_n_vars <- which.min(mse$test)

print(paste("Minimum test error achieved with", min_test_error_n_vars, "variables."))

```

The minimum test error was achieved with 10 variables, in addition to the intercept. Recall that
the actual model equation uses 10 non-zero variables as well.

#### f
\
```{r question_10_f}
print(beta)

print(coef(exhaustive_model, 10))

```

Looks like the correct variables were chosen, with coefficients pretty close to the true values.



#### g
\
```{r question_10_g}

coef_rse <- rep(0, ncols)

for (i in seq_along(coef_rse)) {
  full_coefs <- setNames(rep(0, length(coef_rse)), paste0("X", seq_along(coef_rse)))
  coefs <- coef(exhaustive_model, i)
  coefs <- coefs[setdiff(names(coefs), "(Intercept)")]
  idx <- names(full_coefs)[names(full_coefs) %in% names(coefs)]
  full_coefs[idx] <- coefs
  coef_rse[[i]] <- sqrt(sum((beta - full_coefs) ^ 2))
}

df_for_plot <- data.frame(
  n_vars  = seq_along(coef_rse),
  coef_rse = coef_rse
)

ggplot2::ggplot(data = df_for_plot) +
  ggplot2::geom_line(ggplot2::aes(x = n_vars, y = coef_rse))

```


We achieve a minimum at 10 variables, just like with the test MSE.

### Question 11

#### a - c
\
```{r question_11_a_c}
set.seed(1)
df_boston <- MASS::Boston

nrows <- nrow(df_boston)
train_idx <- sample(nrows, nrows %/% 2)

df_train <- df_boston[train_idx, ]
df_test <- df_boston[-train_idx, ]

models <- list(
  mod_ols = ols_model$new(),
  mod_pcr = dimensionality_reduction_model$new(method = "pcr"),
  mod_plsr = dimensionality_reduction_model$new(method = "plsr"),
  mod_lasso = glmnet_model$new(alpha = 1),
  mod_ridge = glmnet_model$new(alpha = 0),
  best_subsets_model = subsets_model$new(method = "best_subsets"),
  forward_stepwise_model = subsets_model$new(method = "forward"),
  backward_stepwise_model = subsets_model$new(method = "backward")
)

cross_val_indices <- generate_cross_val_index(df_train, 10, 1)

# Cross-validate and fit models
lapply(models, function(model) model$fit_best_model(df_train, "medv",  cross_val_indices))

# Extract r-squared
cv_r2 <- dplyr::bind_rows(
  lapply(models, function(model) model$cv_r2)
)

best_models <- cv_r2[cv_r2$r2 == max(cv_r2$r2), ]

print(best_models)

df_test_without_y <- df_test %>%
  dplyr::select(., -medv)

test_r_2 <- lapply(
  models,
  function(model, df, y) {
    preds <- model$predict(df_test_without_y)
    calculate_r_squared(y, preds)
  },
  df = df_test_without_y,
  y = df_test$medv
)


print(test_r_2)

```
